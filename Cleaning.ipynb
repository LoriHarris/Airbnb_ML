{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\lorie\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "C:\\Users\\lorie\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pymongo \n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "client = MongoClient(\"mongodb://Lori:Les4783!@ds223756.mlab.com:23756/heroku_r58qkhd7\")\n",
    "\n",
    "db = client[\"heroku_r58qkhd7\"]\n",
    "collection = db[\"model\"]\n",
    "import geopy\n",
    "from geopy import distance\n",
    "from geopy.distance import vincenty\n",
    "# from tqdm import tqdm\n",
    "# tqdm.pandas()\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from dateutil import parser\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "import numpy as np\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "# from skopt.space import Real\n",
    "import matplotlib.colors as clt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Change pandas viewing options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No dataset in HDF5 file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9fcd066887f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# NO_reviews2.rename(columns={\"listing_id\": \"id\"}, inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# NO_listRev= pd.merge(cleaned_NO, NO_reviews2, on=\"id\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mstore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'No_reviews'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No dataset in HDF5 file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[0mcandidate_only_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No dataset in HDF5 file."
     ]
    }
   ],
   "source": [
    "# import h5py, os\n",
    "store=pd.HDFStore('NO_reviews.h5')\n",
    "\n",
    "# NO_avgReview = NO_reviews.groupby(\"listing_id\").mean()\n",
    "# NO_avgReview.drop([\"id\", \"reviewer_id\"], axis=1, inplace=True)\n",
    "# NO_reviews2.rename(columns={\"listing_id\": \"id\"}, inplace=True)\n",
    "# NO_listRev= pd.merge(cleaned_NO, NO_reviews2, on=\"id\")\n",
    "store=pd.read_hdf(store)\n",
    "store['No_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('listings.csv.gz', compression='gzip')\n",
    "other_df = pd.read_csv('listings.csv')\n",
    "reviews = pd.read_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = reviews.join(train1, on='id', lsuffix='id')\n",
    "# train['polarity'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lon = -90.0680352\n",
    "lat = 29.9585246\n",
    "train['distance_center'] = train.apply(lambda x: vincenty((x['latitude'], x['longitude']), (lat, lon)).miles, axis = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['price']=(train['price'].replace( '[\\$,)]','', regex=True )\n",
    "                   .replace( '[(]','-',   regex=True ).astype(float))\n",
    "train['cleaning_fee']=(train['cleaning_fee'].replace( '[\\$,)]','', regex=True )\n",
    "                   .replace( '[(]','-',   regex=True ).astype(float))\n",
    "train['security_deposit']=(train['security_deposit'].replace( '[\\$,)]','', regex=True )\n",
    "                   .replace( '[(]','-',   regex=True ).astype(float))\n",
    "sample = train.sample(1000, random_state=42)\n",
    "\n",
    "train['price'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "cleanmean=train['cleaning_fee'].dropna().mean()\n",
    "train['cleaning_fee']=train['cleaning_fee'].fillna(cleanmean)\n",
    "train['square_feet']=train['square_feet'].fillna(train['square_feet'].dropna().mean())\n",
    "train['price']=train['price'].fillna(train['price'].dropna().mean())\n",
    "train['bathrooms']=train['bathrooms'].fillna(train['bathrooms'].dropna().mean())\n",
    "train['bedrooms']=train['bedrooms'].fillna(train['bedrooms'].dropna().mean())\n",
    "train['beds']=train['beds'].fillna(train['beds'].dropna().mean())\n",
    "train['square_feet']=train['square_feet'].fillna(train['square_feet'].dropna().mean())\n",
    "train['security_deposit']=train['security_deposit'].fillna(train['security_deposit'].dropna().mean())\n",
    "train['minimum_nights']=train['minimum_nights'].fillna(train['minimum_nights'].dropna().mean())\n",
    "\n",
    "train['reviews_per_month']=train['reviews_per_month'].fillna(train['reviews_per_month'].dropna().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['price'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.drop(columns=['listing_url','last_scraped','thumbnail_url', 'medium_url','picture_url','xl_picture_url','host_url','host_thumbnail_url','host_picture_url', 'neighbourhood','neighbourhood_group_cleansed','summary', 'neighborhood_overview','scrape_id','host_name', 'id', 'host_id', 'latitude', 'longitude', 'last_review'])\n",
    "label_encoder=LabelEncoder()\n",
    "label_encoder.fit(df['neighbourhood_cleansed'])\n",
    "df['neighbourhood_cleansed']=label_encoder.transform(df['neighbourhood_cleansed'])\n",
    "label_encoder.fit(df['minimum_nights'])\n",
    "df['minimum_nights']=label_encoder.transform(df['minimum_nights'])\n",
    "\n",
    "label_encoder.fit(df['square_feet'])\n",
    "df['square_feet']=label_encoder.transform(df['square_feet'])\n",
    "label_encoder.fit(df['property_type'])\n",
    "df['property_type']=label_encoder.transform(df['property_type'])\n",
    "label_encoder.fit(df['room_type'])\n",
    "df['room_type']=label_encoder.transform(df['room_type'])\n",
    "label_encoder.fit(df['distance_center'])\n",
    "df['distance_center']=label_encoder.transform(df['distance_center'])\n",
    "label_encoder.fit(df['cleaning_fee'])\n",
    "df['cleaning_fee']=label_encoder.transform(df['cleaning_fee'])\n",
    "label_encoder.fit(df['bathrooms'])\n",
    "df['bathrooms']=label_encoder.transform(df['bathrooms'])\n",
    "label_encoder.fit(df['bedrooms'])\n",
    "df['bedrooms']=label_encoder.transform(df['bedrooms'])\n",
    "label_encoder.fit(df['beds'])\n",
    "df['beds']=label_encoder.transform(df['beds'])\n",
    "label_encoder.fit(df['host_listings_count'])\n",
    "df['host_listings_count']=label_encoder.transform(df['host_listings_count'])\n",
    "label_encoder.fit(df['security_deposit'])\n",
    "df['security_deposit']=label_encoder.transform(df['security_deposit'])\n",
    "label_encoder.fit(df['guests_included'])\n",
    "df['guests_included']=label_encoder.transform(df['guests_included'])\n",
    "label_encoder.fit(df['number_of_reviews'])\n",
    "df['number_of_reviews']=label_encoder.transform(df['number_of_reviews'])\n",
    "label_encoder.fit(df['reviews_per_month'])\n",
    "df['reviews_per_month']=label_encoder.transform(df['reviews_per_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['name'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins4 = [50, 100, 200, 350,500, 600,10000]\n",
    "df['price'] = np.searchsorted(bins4, df['price'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({\n",
    "    'MinNights':df['minimum_nights'],\n",
    "    'Hood': df['neighbourhood_cleansed'],\n",
    "    'SqFt':df['square_feet'],\n",
    "    '#Reviews':df['number_of_reviews'],\n",
    "    '#Guests':df['guests_included'],\n",
    "    'Listing Count':df['host_listings_count'],\n",
    "    'Security':df['security_deposit'],\n",
    "    'CleanFee':df['cleaning_fee'],\n",
    "    'Dist':df['distance_center'],\n",
    "#     'Room':df['room_type'],\n",
    "    'BA':df['bathrooms'], \n",
    "#     'Prop':df['property_type'],\n",
    "    'BR':df['bedrooms'], \n",
    "    'Beds': df['beds'],\n",
    "    'Acc':df['accommodates'],\n",
    "    'Price':df['price'],\n",
    "    '#Booked':df['reviews_per_month'],\n",
    "    'Name':df['name']\n",
    "})\n",
    "\n",
    "# train1 = train.dropna()\n",
    "test.to_csv('test_data.csv')\n",
    "# train1['Hood'].unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby(['neighbourhood_cleansed']).mean()\n",
    "price_grouped = other_df.groupby(['neighbourhood']).mean()\n",
    "price = price_grouped['price']\n",
    "# df.head()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['price'].head()\n",
    "# price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_high = df1.loc[(df1['price']>=225)]\n",
    "price_mid = df1[(df1['price']>150) & (df1['price'] <225)]\n",
    "price_low = df1[(df1['price']>=0) & (df1['price'] <150)]\n",
    "price_all = df1[df1['price']>1]\n",
    "p_low = price_low.reset_index()\n",
    "p_mid = price_mid.reset_index()\n",
    "p_high = price_high.reset_index()\n",
    "p_all = price_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_price(feature, index):\n",
    "    fig, ax = plt.subplots()\n",
    "    x_axis = np.arange(len(feature['price']))\n",
    "    ax.bar(x_axis, feature['price'])\n",
    "    labels =index['neighbourhood_cleansed']\n",
    "    ax.set_xticks(x_axis)\n",
    "    ax.set_xticklabels(labels, rotation=90, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bar_price(price_all, p_all)\n",
    "# bar_price(price_low, p_low)\n",
    "# bar_price(price_mid, p_mid)\n",
    "# bar_price(price_high, p_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = \"#000080\"\n",
    "b = \"#00BFFF\"\n",
    "c = \"#32cd32\"\n",
    "d = \"#FF4500\"\n",
    "clt.to_hex(a)\n",
    "clt.to_hex(b)\n",
    "clt.to_hex(c)\n",
    "clt.to_hex(d)\n",
    "\n",
    "price = sample['price']\n",
    "dist = sample['distance_center']\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(price,dist, color=b)\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('${x:,.0f}'))\n",
    "\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.title('Distance versus Price')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Distance')\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.reset_index()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = df.drop(columns=['listing_url','last_scraped','thumbnail_url', 'medium_url','picture_url','xl_picture_url','host_url','host_thumbnail_url','host_picture_url', 'neighbourhood','neighbourhood_group_cleansed','summary', 'neighborhood_overview','scrape_id','host_name','name', 'id', 'host_id', 'latitude', 'longitude', 'last_review', 'reviews_per_month'])\n",
    "\n",
    "\n",
    "train1 = pd.DataFrame({\n",
    "    \n",
    "        'MinNights':df['minimum_nights'],\n",
    "\n",
    "    'Hood': df['neighbourhood_cleansed'],\n",
    "    'SqFt':df['square_feet'],\n",
    "    '#Reviews':df['number_of_reviews'],\n",
    "    '#Guests':df['guests_included'],\n",
    "    'Listing Count':df['host_listings_count'],\n",
    "    'Security':df['security_deposit'],\n",
    "    'CleanFee':df['cleaning_fee'],\n",
    "    'Dist':df['distance_center'],\n",
    "#     'Room':df['room_type'],\n",
    "    'BA':df['bathrooms'], \n",
    "#     'Prop':df['property_type'],\n",
    "    'BR':df['bedrooms'], \n",
    "    'Beds': df['beds'],\n",
    "    'Acc':df['accommodates'],\n",
    "    'Price':df['price'],\n",
    "        '#Booked':df['reviews_per_month']\n",
    "\n",
    "})\n",
    "# train1=train1[:-1]\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bins=[0,25,50,100,150,200]\n",
    "# labels=[1,2,3,4,5]\n",
    "# # train1['CleanFee'] = pd.cut(train1['CleanFee'], bins=bins, labels=labels)\n",
    "# train1['CleanFee'] = np.searchsorted(bins, train1['CleanFee'].values)\n",
    "\n",
    "# bins2=[1000,2000,3000,4000,5000,6000]\n",
    "# labels2=[.5,1,1.5,2,2.5,3]\n",
    "# train1['Dist'] = np.searchsorted(bins2, train1['Dist'].values)\n",
    "\n",
    "# # train1['Dist'] = pd.cut(train1['Dist'], bins=bins2, labels=labels2)\n",
    "\n",
    "# bins1 = [50,100, 150, 250,300,400, 600, 1000, 8000]\n",
    "# labels1 = [1,2,3,4, 5, 6,7, 8, 9]\n",
    "# train1['Price'] = np.searchsorted(bins1, train1['Price'].values)\n",
    "# # train1['Price'] = pd.cut(train1['Price'], bins=bins1, labels=labels1)\n",
    "# bins3=[50,100,150,250,300,500]\n",
    "\n",
    "# train1['#Reviews'] = np.searchsorted(bins3, train1['#Reviews'].values)\n",
    "# bins4 = [200, 300, 400,500, 600,10000]\n",
    "# labels4 = [1,2,3,4, 5, 6,7, 8, 9]\n",
    "# train1['Price'] = np.searchsorted(bins4, train1['Price'].values)\n",
    "\n",
    "# train1['Dist']=train1['Dist'].fillna(2)\n",
    "# train1['CleanFee']=train1['CleanFee'].fillna(2)\n",
    "# train1.to_csv('test_data.csv')\n",
    "# # train1['Hood'].unique\n",
    "# train1['#Booked']\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x = train1.drop('Price', axis=1)\n",
    "y = train1['Price']\n",
    "print(x.shape, y.shape)\n",
    "y.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1.groupby(['#Booked']).min()\n",
    "train1['#Booked'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.5, stratify=y)\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.fit_transform(y_train)\n",
    "encoded_y_test = label_encoder.fit_transform(y_test)\n",
    "# y_scaler = StandardScaler()\n",
    "# encoded_y_train = (y_train)\n",
    "# encoded_y_test = (y_test)\n",
    "\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)\n",
    "print(x_train_scaled.shape, y_train_categorical.shape)\n",
    "print(x_test_scaled.shape, y_test_categorical.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "k_fold = KFold(n_splits=10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors =5)\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, x, y, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, x, y, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=11)\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, x, y, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, x, y, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SVC()\n",
    "# scoring = 'accuracy'\n",
    "# score = cross_val_score(clf, x, y, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "\n",
    "# print(score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, Conv2D, Activation, Reshape\n",
    "\n",
    "# Create model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu',input_dim=x_train_scaled.shape[1]))\n",
    "model.add(Dense(units=1000, activation='relu'))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "\n",
    "model.add(Dense(units=y_train_categorical.shape[1], activation='softmax'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "#                 optimizer='adam',\n",
    "                optimizer='adadelta',\n",
    "#               loss='categorical_crossentropy',\n",
    "              loss='mse',\n",
    "#                  metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "              metrics=['accuracy'])\n",
    "# from keras.optimizers import SGD\n",
    "# opt = SGD(lr=0.01)\n",
    "# model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=5,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.evaluate(x_test_scaled, y_test_categorical, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_train_scaled, y_train_categorical, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_predictions = model.predict_classes(x_test_scaled[:5])\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)\n",
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {list(y_test[:5])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"airbnb.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1=load_model(\"airbnb.h5\")\n",
    "testing = pd.read_csv('test_data.csv')\n",
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1=testing.drop('Unnamed: 0', axis=1)\n",
    "test_data=test_data1.drop('Name', axis=1)\n",
    "\n",
    "price=test_data['Price']\n",
    "test_data=test_data.drop('Price', axis=1)\n",
    "\n",
    "x_scaler=StandardScaler().fit(test_data)\n",
    "x_test_scaled1 = x_scaler.transform(test_data)\n",
    "prediction=model1.predict_classes(x_test_scaled1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(price)\n",
    "y_test1 = label_encoder.fit_transform(price)\n",
    "encoded_predictions1 = model1.predict_classes(x_test_scaled1[:6])\n",
    "prediction_labels1 = label_encoder.inverse_transform(encoded_predictions1)\n",
    "print(f\"Predicted classes: {prediction_labels1}\")\n",
    "print(f\"Actual Labels: {list(y_test1[:6])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data.head()\n",
    "submission = pd.DataFrame({\"Prediction\":prediction_labels1[:6], \"Booked\":y_test1[:6]})\n",
    "submission.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = GradientBoostingRegressor(n_estimators=75, learning_rate=0.17, max_depth=5, subsample=1.0,\n",
    "                                 random_state=42)\n",
    "regr.fit(x_train, y_train)\n",
    "print(r2_score(y_test, regr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(regr, random_state=42).fit(x_test, y_test)\n",
    "eli5.show_weights(perm, top=x.shape[1], feature_names = x.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {classifier.score(x_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=23)\n",
    "scoring = 'accuracy'\n",
    "score = cross_val_score(clf, x_train, y_train, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "\n",
    "print(score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(x_train,y_train)\n",
    "# import os\n",
    "# from sklearn.tree import export_graphviz\n",
    "# import six\n",
    "# import pydot\n",
    "# from sklearn import tree\n",
    "# dotfile = six.StringIO()\n",
    "# i_tree = 0\n",
    "# for tree_in_forest in clf.estimators_:\n",
    "#     export_graphviz(tree_in_forest,out_file='tree.dot',\n",
    "#     feature_names=x_train.columns,\n",
    "#     filled=True,\n",
    "#     rounded=True)\n",
    "#     (graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "#     name = 'tree' + str(i_tree)\n",
    "#     graph.write_png(name+  '.png')\n",
    "#     os.system('dot -Tpng tree.dot -o tree.png')\n",
    "#     i_tree +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train,y_train)\n",
    "prediction=clf.predict(test_data)\n",
    "submission = pd.DataFrame({\"Prediction\":prediction, \"Booked\":price})\n",
    "submission.to_csv('submission')\n",
    "submission.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and add layers\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(units=100, activation='tanh',input_dim=x_train_scaled.shape[1]))\n",
    "  \n",
    "model1.add(Dense(units=1000, activation='softmax'))\n",
    "model1.add(Dense(units=100, activation='relu'))\n",
    "\n",
    "# model1.add(Dense(units=100, activation='sigmoid'))\n",
    "\n",
    "model1.add(Dense(units=6, activation='softmax'))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(units=2, activation='softmax'))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(\n",
    "                optimizer='adam',\n",
    "#                 optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "#               loss='mse',\n",
    "#                  metrics=['mean_squared_error', 'mean_absolute_error'])\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(\n",
    "   x_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=5,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(x_test_scaled, y_test_categorical, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.evaluate(x_train_scaled, y_train_categorical, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"airbnb1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1=load_model(\"airbnb1.h5\")\n",
    "testing = pd.read_csv('test_data.csv')\n",
    "testing.head()\n",
    "test_data=testing.drop('Unnamed: 0', axis=1)\n",
    "price=test_data['#Booked']\n",
    "test_data=test_data.drop('#Booked', axis=1)\n",
    "\n",
    "x_scaler=StandardScaler().fit(test_data)\n",
    "x_test_scaled = x_scaler.transform(test_data)\n",
    "prediction=model1.predict_classes(x_test_scaled)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(price)\n",
    "y_test = label_encoder.fit_transform(price)\n",
    "encoded_predictions = model1.predict_classes(x_test_scaled[:6])\n",
    "prediction_labels = label_encoder.inverse_transform(encoded_predictions)\n",
    "print(f\"Predicted classes: {prediction_labels}\")\n",
    "print(f\"Actual Labels: {list(y_test[:6])}\")\n",
    "submission = pd.DataFrame({\"Prediction\":prediction_labels[:6], \"Price\":y_test[:6]})\n",
    "submission.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "# from IPython import display\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "(str_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
